{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.xception import Xception\n",
    "from keras.layers import Dense,Flatten\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import layers\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Activation,GlobalMaxPooling2D,GlobalAveragePooling2D,AveragePooling2D\n",
    "from keras import applications\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8912\n"
     ]
    }
   ],
   "source": [
    "capture = cv2.VideoCapture(\"C:\\\\Users\\\\KUSHAL MASTER\\\\Downloads\\\\Dataset-2\\\\Train Tom and jerry.mp4\")\n",
    "length = int(capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "print( length )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,length + 1):\n",
    "    ret, frame = capture.read()\n",
    "    cv2.imwrite('C:\\\\Users\\\\KUSHAL MASTER\\\\Downloads\\\\Dataset-2\\\\Train Images\\\\frame' + str(i) + '.jpg', frame)\n",
    "capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = pd.read_csv(\"C:/Users/KUSHAL MASTER/Downloads/Dataset-2/train.csv\")\n",
    "training_imgs = [ x for x in list(training_set.Frame_ID)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['frame0.jpg', 'frame1.jpg', 'frame2.jpg', 'frame3.jpg', 'frame4.jpg', 'frame5.jpg', 'frame6.jpg', 'frame7.jpg', 'frame8.jpg', 'frame9.jpg', 'frame10.jpg', 'frame11.jpg', 'frame12.jpg', 'frame13.jpg', 'frame14.jpg', 'frame15.jpg', 'frame16.jpg', 'frame17.jpg', 'frame18.jpg', 'frame19.jpg', 'frame20.jpg', 'frame21.jpg', 'frame22.jpg', 'frame23.jpg', 'frame24.jpg', 'frame25.jpg', 'frame26.jpg', 'frame27.jpg', 'frame28.jpg', 'frame29.jpg', 'frame30.jpg', 'frame31.jpg', 'frame32.jpg', 'frame33.jpg', 'frame34.jpg', 'frame35.jpg', 'frame36.jpg', 'frame37.jpg', 'frame38.jpg', 'frame39.jpg', 'frame40.jpg', 'frame41.jpg', 'frame42.jpg', 'frame43.jpg', 'frame44.jpg', 'frame45.jpg', 'frame46.jpg', 'frame47.jpg', 'frame48.jpg', 'frame49.jpg', 'frame50.jpg', 'frame51.jpg', 'frame52.jpg', 'frame53.jpg', 'frame54.jpg', 'frame55.jpg', 'frame56.jpg', 'frame57.jpg', 'frame58.jpg', 'frame59.jpg', 'frame60.jpg', 'frame61.jpg', 'frame62.jpg', 'frame63.jpg', 'frame64.jpg', 'frame65.jpg', 'frame66.jpg', 'frame67.jpg', 'frame68.jpg', 'frame69.jpg', 'frame70.jpg', 'frame71.jpg', 'frame72.jpg', 'frame73.jpg', 'frame74.jpg', 'frame75.jpg', 'frame76.jpg', 'frame77.jpg', 'frame78.jpg', 'frame79.jpg', 'frame80.jpg', 'frame81.jpg', 'frame82.jpg', 'frame83.jpg', 'frame84.jpg', 'frame85.jpg', 'frame86.jpg', 'frame87.jpg', 'frame88.jpg', 'frame89.jpg', 'frame90.jpg', 'frame91.jpg', 'frame92.jpg', 'frame93.jpg', 'frame94.jpg', 'frame94.jpg', 'frame96.jpg', 'frame97.jpg', 'frame98.jpg', 'frame99.jpg', 'frame100.jpg', 'frame101.jpg', 'frame102.jpg', 'frame103.jpg', 'frame104.jpg', 'frame105.jpg', 'frame106.jpg', 'frame107.jpg', 'frame108.jpg', 'frame109.jpg', 'frame110.jpg', 'frame111.jpg', 'frame112.jpg', 'frame113.jpg', 'frame114.jpg', 'frame115.jpg', 'frame116.jpg', 'frame117.jpg', 'frame118.jpg', 'frame119.jpg', 'frame120.jpg', 'frame121.jpg', 'frame122.jpg', 'frame123.jpg', 'frame124.jpg', 'frame125.jpg', 'frame126.jpg', 'frame127.jpg', 'frame128.jpg', 'frame129.jpg', 'frame130.jpg', 'frame131.jpg', 'frame132.jpg', 'frame133.jpg', 'frame134.jpg', 'frame135.jpg', 'frame136.jpg', 'frame137.jpg', 'frame138.jpg', 'frame139.jpg', 'frame140.jpg', 'frame141.jpg', 'frame142.jpg', 'frame143.jpg', 'frame144.jpg', 'frame145.jpg', 'frame146.jpg', 'frame147.jpg', 'frame148.jpg', 'frame149.jpg', 'frame150.jpg', 'frame151.jpg', 'frame152.jpg', 'frame153.jpg', 'frame154.jpg', 'frame155.jpg', 'frame156.jpg', 'frame157.jpg', 'frame158.jpg', 'frame159.jpg', 'frame160.jpg', 'frame161.jpg', 'frame162.jpg', 'frame163.jpg', 'frame164.jpg', 'frame165.jpg', 'frame166.jpg', 'frame167.jpg', 'frame168.jpg', 'frame169.jpg', 'frame170.jpg', 'frame171.jpg', 'frame172.jpg', 'frame173.jpg', 'frame174.jpg', 'frame175.jpg', 'frame176.jpg', 'frame177.jpg', 'frame178.jpg', 'frame179.jpg', 'frame180.jpg', 'frame181.jpg', 'frame182.jpg', 'frame183.jpg', 'frame184.jpg', 'frame185.jpg', 'frame186.jpg', 'frame187.jpg', 'frame188.jpg', 'frame189.jpg', 'frame190.jpg', 'frame191.jpg', 'frame192.jpg', 'frame193.jpg', 'frame194.jpg', 'frame195.jpg', 'frame196.jpg', 'frame197.jpg', 'frame198.jpg', 'frame199.jpg', 'frame200.jpg', 'frame201.jpg', 'frame202.jpg', 'frame203.jpg', 'frame204.jpg', 'frame205.jpg', 'frame206.jpg', 'frame207.jpg', 'frame208.jpg', 'frame209.jpg', 'frame210.jpg', 'frame211.jpg', 'frame212.jpg', 'frame213.jpg', 'frame214.jpg', 'frame215.jpg', 'frame216.jpg', 'frame217.jpg', 'frame218.jpg', 'frame219.jpg', 'frame220.jpg', 'frame221.jpg', 'frame222.jpg', 'frame223.jpg', 'frame224.jpg', 'frame225.jpg', 'frame226.jpg', 'frame227.jpg', 'frame228.jpg', 'frame229.jpg', 'frame230.jpg', 'frame231.jpg', 'frame232.jpg', 'frame233.jpg', 'frame234.jpg', 'frame235.jpg', 'frame236.jpg', 'frame237.jpg', 'frame238.jpg', 'frame239.jpg', 'frame240.jpg', 'frame241.jpg', 'frame242.jpg', 'frame243.jpg', 'frame244.jpg', 'frame245.jpg', 'frame246.jpg', 'frame247.jpg', 'frame248.jpg', 'frame249.jpg', 'frame250.jpg', 'frame251.jpg', 'frame252.jpg', 'frame253.jpg', 'frame254.jpg', 'frame255.jpg', 'frame256.jpg', 'frame257.jpg', 'frame258.jpg', 'frame259.jpg', 'frame260.jpg', 'frame261.jpg', 'frame262.jpg', 'frame263.jpg', 'frame264.jpg', 'frame265.jpg', 'frame266.jpg', 'frame267.jpg', 'frame268.jpg', 'frame269.jpg', 'frame270.jpg', 'frame271.jpg', 'frame272.jpg', 'frame273.jpg', 'frame274.jpg', 'frame275.jpg', 'frame276.jpg', 'frame277.jpg', 'frame278.jpg', 'frame279.jpg', 'frame280.jpg', 'frame281.jpg', 'frame282.jpg', 'frame283.jpg', 'frame284.jpg', 'frame285.jpg', 'frame286.jpg', 'frame287.jpg', 'frame288.jpg', 'frame289.jpg', 'frame290.jpg', 'frame291.jpg', 'frame292.jpg', 'frame293.jpg', 'frame294.jpg', 'frame295.jpg', 'frame296.jpg', 'frame297.jpg']\n"
     ]
    }
   ],
   "source": [
    "print(training_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Frame_ID Emotion\n",
      "0      frame0.jpg       0\n",
      "1      frame1.jpg       0\n",
      "2      frame2.jpg       1\n",
      "3      frame3.jpg       1\n",
      "4      frame4.jpg       2\n",
      "..            ...     ...\n",
      "293  frame293.jpg       3\n",
      "294  frame294.jpg       3\n",
      "295  frame295.jpg       3\n",
      "296  frame296.jpg       3\n",
      "297  frame297.jpg       3\n",
      "\n",
      "[298 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "training_set.Emotion = pd.factorize(training_set.Emotion)[0]\n",
    "training_set.Emotion = training_set.Emotion.astype(str)\n",
    "print(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_labels = list(training_set['Emotion'])\n",
    "training_set = pd.DataFrame( {'Frame_ID': training_imgs,'Emotion': training_labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Frame_ID Emotion\n",
      "0  frame0.jpg       0\n",
      "1  frame1.jpg       0\n",
      "2  frame2.jpg       1\n",
      "3  frame3.jpg       1\n",
      "4  frame4.jpg       2\n"
     ]
    }
   ],
   "source": [
    "print(training_set.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv(\"C:/Users/KUSHAL MASTER/Downloads/dataset/train.csv\")\n",
    "#print(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "train_dataGen = ImageDataGenerator(rescale = 1./255,\n",
    "                                  shear_range = 0.2,\n",
    "                                  zoom_range = 0.2,\n",
    "                                  horizontal_flip = True)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 298 validated image filenames belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"train_generator = train_dataGen.flow_from_dataframe(\n",
    "                                        dataframe = training_set,\n",
    "                                        directory=\"C:\\\\Users\\\\KUSHAL MASTER\\\\Downloads\\\\Dataset-2\\\\Train Images\",x_col=\"Frame_ID\",\n",
    "                                        y_col=\"Emotion\",\n",
    "                                        class_mode=\"categorical\",\n",
    "                                        target_size=(128,128),\n",
    "                                        batch_size=64)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataGen = ImageDataGenerator(rescale=1./255,rotation_range=30,\n",
    "\tzoom_range=0.15,\n",
    "\twidth_shift_range=0.2,\n",
    "\theight_shift_range=0.2,\n",
    "\tshear_range=0.15,\n",
    "\thorizontal_flip=True,\n",
    "\tfill_mode=\"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"train_generator = train_datagen.flow_from_directory('C:\\\\Users\\\\KUSHAL MASTER\\\\Downloads\\\\Dataset-2\\\\Train Images',target_size=(299,299),batch_size = 16,seed=np.random.seed())\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 298 validated image filenames belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_dataGen.flow_from_dataframe(\n",
    "                                        dataframe = training_set,\n",
    "                                        directory=\"C:\\\\Users\\\\KUSHAL MASTER\\\\Downloads\\\\Dataset-2\\\\Train Images\",x_col=\"Frame_ID\",\n",
    "                                        y_col=\"Emotion\",\n",
    "                                        class_mode=\"categorical\",\n",
    "                                        target_size=(299,299),\n",
    "                                        batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "classifier = Sequential()\n",
    "#First Convolutional layer\n",
    "classifier.add(Conv2D(filters = 56,kernel_size = (3,3), activation = 'relu', input_shape = (128,128,3)))\n",
    "classifier.add(MaxPooling2D(pool_size = (2,2)))\n",
    "#second Convolutional layer\n",
    "classifier.add(Conv2D(32,(3,3),activation = 'relu'))\n",
    "classifier.add(MaxPooling2D(pool_size = (2,2)))\n",
    "#Flattening\n",
    "classifier.add(Flatten())\n",
    "#Hidden Layer\n",
    "classifier.add(Dense(units = 64, activation = 'relu'))\n",
    "#Output Layer\n",
    "classifier.add(Dense(units = 4 , activation = 'softmax'))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(128,128,3)))\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(5, activation='softmax'))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''classifier.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['categorical_accuracy','accuracy'])\n",
    "classifier.summary()\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"model.compile(optimizer= 'adam', loss='categorical_crossentropy', metrics=['categorical_accuracy','accuracy'])\\nmodel.summary()\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compile model\n",
    "\"\"\"model.compile(optimizer= 'adam', loss='categorical_crossentropy', metrics=['categorical_accuracy','accuracy'])\n",
    "model.summary()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model.fit_generator(train_generator, epochs = 50, steps_per_epoch = 50, verbose=1)'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"model.fit_generator(train_generator, epochs = 50, steps_per_epoch = 50, verbose=1)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"model = Sequential()\n",
    "\n",
    "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu', input_shape=(128,128,3)))\n",
    "model.add(Conv2D(128,kernel_size= (3, 3), activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "model.add(Conv2D(512, (3, 3), activation='relu'))\n",
    "model.add(Conv2D(512, (3, 3), activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "#fully connected neural networks\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(5, activation='softmax'))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"model.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['categorical_accuracy','accuracy'])\\nmodel.summary()\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"model.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['categorical_accuracy','accuracy'])\n",
    "model.summary()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    }
   ],
   "source": [
    "\"\"\"model.fit_generator(train_generator, epochs = 50, steps_per_epoch = 50, verbose=1)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = Xception(weights='imagenet',include_top=False,input_shape=(299, 299, 3)) #include_top ---> not keeping complete Model\n",
    "\n",
    "headModel = base_model.output\n",
    "headModel = AveragePooling2D(pool_size=(4, 4))(headModel)\n",
    "headModel = Flatten(name=\"flatten\")(headModel)\n",
    "headModel = Dense(128, activation=\"relu\")(headModel)\n",
    "headModel = Dropout(0.5)(headModel)\n",
    "headModel = Dense(5, activation=\"softmax\")(headModel)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=headModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optimizers.Adam(lr=1e-4)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=opt,metrics=[\"categorical_accuracy\",\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[64,147,147,128] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n\t [[{{node block2_sepconv2_1/separable_conv2d}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-89450dfbf5d4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m298\u001b[0m\u001b[1;33m//\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1730\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1731\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1732\u001b[1;33m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1733\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m    218\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m                                             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 220\u001b[1;33m                                             reset_metrics=False)\n\u001b[0m\u001b[0;32m    221\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[0;32m   1512\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1513\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1514\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1515\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1516\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3474\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3475\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3476\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3477\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3478\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1472\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1473\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[64,147,147,128] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n\t [[{{node block2_sepconv2_1/separable_conv2d}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n"
     ]
    }
   ],
   "source": [
    "model.fit_generator(train_generator,epochs=5,steps_per_epoch=298//16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = pd.read_csv(\"C:/Users/KUSHAL MASTER/Downloads/dataset/test.csv\")\n",
    "test_imgs = [x for x in list(test_set.Image)]\n",
    "test_set = pd.DataFrame( {'Image': test_imgs })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "\n",
    "Y_pred = []\n",
    "\n",
    "for i in range(len(test_set)):\n",
    "    img = image.load_img(path=\"C:/Users/KUSHAL MASTER/Downloads/dataset/Test Images/\" + test_set.Image[i] ,target_size=(128,128,3))\n",
    "    img = image.img_to_array(img)\n",
    "    test_img = img.reshape((1,128,128,3))\n",
    "    img_class = model.predict_classes(test_img)\n",
    "    prediction = img_class[0]\n",
    "    Y_pred.append(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = []\n",
    "for i in range(0, len(Y_pred)):\n",
    "    if (Y_pred[i] == 0):\n",
    "        predicted.append(\"Food\")\n",
    "    elif (Y_pred[i] == 1):\n",
    "        predicted.append(\"misc\")\n",
    "    elif (Y_pred[i] == 2):\n",
    "        predicted.append(\"Attire\")\n",
    "    else:\n",
    "        predicted.append(\"Decorationandsignage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(list(zip(test_set.Image, predicted)),columns=[\"Image\",\"Class\"]).to_csv(\"C:/Users/KUSHAL MASTER/Downloads/dataset/submission.csv\",index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
